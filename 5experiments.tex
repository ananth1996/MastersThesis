\chapter{Experiments}
\label{chp:experiments}
In this chapter, we will outline the experiments carried out on the Wikipedia elections of administrators using the vote predicition models that we presented in chapter~\ref{chp:vote-prediction}.
Firstly, we describe the existing sources of data from Wikipedia and the datasets used in the experiments in Section~\ref{sec:datasets}.
Next, in Section~\ref{sec:linear-combination-implementation}, we discuss the implementation of the linear combination of graph model described in Section~\ref{sec:linear-combination-theory}.
Then, we cover the implementation of the vote predicition models based on the theories of balance and status in signed networks in Section~\ref{sec:local-signed-network-implementation}.
Furthermore, in Section~\ref{sec:voting-order}, we discuss the experiments conducted on the voting order and its impact on the predictive power of the models proposed.
Lastly, we explain the metrics which we can ue to evaluate the performance of the models in Section~\ref{sec:eval-metrics}.


\section{Datasets}
\label{sec:datasets}
As we discussed in Section~\ref{sec:rfa} and \ref{subsec:editors}, Wikipedia keeps detailed information on the election proceedings for the RfA process as well as contributions made by every editor on Wikipedia.
These act as sources to get data regarding the elections and user contributions. There are existing datasets compiled by the Stanford Network Analysis Project (SNAP) \cite{snapnets} on both Wikipedia RfAs and edit histories.
However, the RfA dataset has missing features and timestamps for votes that would restrict the usability in the proposed voting models.
Similarly, the \textit{wiki-meta} and \textit{wiki-talk} datasets possess information until 2008 and lack a username mapping to the network nodes.
Due to these limitations, we proceeded to scrape Wikipedia dumps and APIs to obtain our own RfA and user contribution datasets, which we will now describe.

\subsection{Wikipedia RfA Data}
To obtain the RfA data, we parsed through the entire XML dump of Wikipedia from January 2019.
We filtered the pages related to the RfA process and then extracted each vote and the corresponding comment and timestamp.
Each vote extracted has the features show in Table~\ref{tab:wiki-rfa-features}.

\begin{table}[htp]
    \centering
    \caption{Features of each vote in the \wikirfa dataset}
    \label{tab:wiki-rfa-features}
    \begin{tabular}{ccc}
        \toprule
        Feature & Data Type & Description\\
        \midrule
        \textbf{SRC}&text & username of the source\\
        \textbf{TGT}&text & username of the target\\
        \textbf{VOT}&$[-1,0,1]$& Oppose, Neutral or Support vote\\
        \textbf{RES}&$[-1,1]$ & Failure or Success of RfA\\
        \textbf{YEA}&date & year of the RfA\\
        \textbf{DAT}& date \& time & timestamp of the vote\\
        \textbf{TXT}&text &accompanying textual comment \\
        \textbf{UID}&alphanumeric&  unique identifier for the RfA\\
        \bottomrule
    \end{tabular}
\end{table}
As we can see, the format of the data is very similar to the SNAP dataset.
We have an additional unique identifier field, called UID, to aid in distinguishing RfA of users who have had multiple nominations.
We collected $226\,781$ votes from $4\,557$ elections with over $13\,000$ unique usernames. There are $166\,214$ ($\approx 73\%$) support, $46\,918$ ($\approx 20\%$) oppose and $13\,649$ ($\approx 6\%$) neutral votes.
As the voting format of RfA changes throughout the years, there might be issues in successfully extracting the source username or timestamp information. 
Regardless, only $1.6\%$ of votes have missing timestamps and $0.4\%$ have a missing source.
We will refer to this dataset as \wikirfa and it will provide the information regarding the votes cast in a RfA.

\subsection{User Contribution Data}
As we discussed in Section~\ref{subsec:editors}, every edit made by a user is stored as a contribution.
Wikipedia provides an API to query all the contributions of a particular user \cite{wiki:Usercontribs-api}.  
We utilized this API and collected the contribution data of all the unique users we obtained from the \wikirfa dataset.
There are 16 features that the API provides for each edit; we describe the most import features in Table~\ref{tab:usercontrib-features}.

\begin{table}[htp]
    \centering
    \caption{Important features of each contribution in the \usercontrib dataset}
    \label{tab:usercontrib-features}
    \begin{tabular}{lcc}
        \toprule
        Feature & Data Type & Description\\
        \midrule
        \textbf{user}&text& username of the editor\\
        \textbf{title}&text & title of the page edited\\
        \textbf{namespace}&int& namespace of the page edited\\
        \textbf{timestamp}&date \& time & timestamp of the edit\\
        \textbf{size}&int& new size of the edit \\
        \textbf{sizediff}& int & size delta of the edit against its parent\\
        \textbf{new}&boolean &if the editor created a new page \\
        \textbf{minor}&boolean& if it is a minor edit\\
        \textbf{comment}& text& accompanying comment\\
        \bottomrule
    \end{tabular}
\end{table}
As many users change their username, some of the usernames present in the \wikirfa dataset might not have any contributions linked to their old usernames.
We were able to collect the user contribution details more than $11000$ of the total unique username, amounting to 100GB of data.
We call this dataset \usercontrib and it provides a wealth of information on the editing habits of the users who take part in Wikipedia RfAs. 
For instance, grouping the contributions of a particular user by the namespace, we get the proportion of the edits in different Wikipedia namespaces and the respective sizes and quality of their edits.

\section{Linear Combination of Graphs Model}
\label{sec:linear-combination-implementation}
In this section, we describe how we implement the model proposed in Section~\ref{sec:linear-combination-theory}.
The model requires auxiliary graphs derived from other non-election based information as well as triadic features derived from the election data.
Therefore, we first discuss the auxiliary graphs that we create from the \usercontrib dataset and then describe the triadic features from the \wikirfa data.
Then, we describe the process of preparing the data to suit the supervised machine learning task and to also prevent data leaks.
Lastly, we discuss the logistic regression model that we use as the linear predictor trained on the features derived from the auxiliary and signed networks.

The terms used in Chapter~\ref{chp:vote-prediction} can now be defined for the problem of predicting votes in a Wikipedia RfA.
A candidate $c$ is the nominee who wishes to gain administrators privileges in the Wikipedia RfA.
The voters $v$ are the registered users in Wikipedia.
A session relates to the proceedings of a particular RfA.
\subsection{Graphs}
First, we discuss the creation of the topic similarity network of users.
Then, we describe the process of forming the talk graph between users.
Lastly, we define the triadic features we extract from the previous voting data. 

\subsubsection{Topic Similarly Graph}
In Table~\ref{tab:usercontrib-features}, we see that every contribution has a title of the page where the edit was made.
We can utilize the page titles to understand the topics that the user most edits.
For a particular user, we gather all their edits in the \mainNS namespace.
We choose the \mainNS namespace as it contains all the content articles on Wikipedia.
By contrast, the edits in other namespaces such as, \userNS and \helpNS, are not indicative of the topics that they are most likely to possess information about.
Then, we count the number of edits grouped by each page title and choose the top 100 most edited pages in the \mainNS namespace.
Then we create a set of the words from all the top 100 page titles and remove common stop words using a natural language corpus.
This set now indicates the users topics of interest.
Once we have collected the topic set for all the unique users in the \wikirfa dataset, we can compute the similarity between a pair of users using the Jaccard similarity measure. 
We can take this similarity measure and construct a undirected weighted graph where a link between nodes indicates the similarity in the topics of the user nodes.
However, we threshold the value of similarity so that we can obtain only meaningful edges and not a complete graph.

\subsubsection{Talk and Interaction Graph}
We discussed in the previous chapter on how every registered user has a talk page and how it is used as a medium of communication. 
Therefore, we can similarly gather the contributions of a certain editor in the \usertalkNS namespace and use it to measure interactions with other users.
We will create two auxiliary graphs in this manner, a \textit{user talk graph}, where each edge also contains the number of times they have written on another users page and a \textit{interaction graph}, which only indicates if two users have interacted via a talk page. 
We can obtain the number of talk page edits and the target user by grouping by the page titles and extracting the username from the page title respectively. 
These graphs will be directed in nature and the talk graph is weighted, while the interaction graph is unweighted. 
In both these graphs, an edge $u \rightarrow v$ indicate that user $u$ has written in the talk page of user $v$.

In the Line~\ref{alg:aux:voting-neighbourhood} described in Algorithm~\ref{alg:auxiliary-feature}, we can define $N_i$ in directed graphs $G_i$ as successors of a node rather the neighbourhood.
This allow us to understand the influence of edge direction in directed auxiliary graphs.
Therefore, we will construct two more additional auxiliary graphs which are reversed, i.e., an edge $u \rightarrow v$ indicates that user $v$ has written on the talk page of user $u$.
Hence, we can compare the contribution each direction brings to the model by analysing the feature importances.

\subsubsection{Signed Graph and Triadic Features}
The \wikirfa dataset contains the voting information of users in RfAs.
These votes form a signed directed network and we can utilize triadic features as proposed by Leskovec et al.\ \cite{leskovec2010predicting}.
We utilize a slightly modified naming scheme to identify unique triads in the RfA data.
Considering the edge we want to predict between a voter $v$ and a candidate $c$ and the other node $u$.
There are two directions for the edges $(v,u)$ and $(u,c)$ and each edge can have three values, namely -1, 0 or +1 corresponding to oppose, neutral or support respectively.
This leads to $2 \times 2 \times 3 \times 3 = 36$ possible triads.

We denote the edge $v \rightarrow u$ as "F" and the edge $v \leftarrow u$ as "B" indicating a forward or a backward edge.
Similarly, the edge $u \rightarrow c$ is "F" and $u \leftarrow c$ is "B".
The edge labels are "-", "0", and "+" corresponding to oppose, neutral and support.
We have a nomenclature where "FB+-", indicates $v\plusrightarrow c$ and $u \minusleftarrow c$.
Figure~\ref{fig:triad-naming} shows more examples of this triad nomenclature.

\begin{figure}[htp]
    \centering
    \input{images/wiki-triads.tex}
    \caption{Examples of triad nomenclature in Wikipedia RfA elections }
    \label{fig:triad-naming}
\end{figure}
We collect all the 36 unique triads in the set $T$ and then utilize it to count the triads for a particular edge, as seen in Algorithm~\ref{alg:triad-feature}.
Therefore, for each edge to be predicted $(v,c)$, we have a triadic feature vector of length 36 containing the counts of the triads formed by all the common neighbours $u$.  

\subsection{Data Preparation}
As only roughly $6\%$ of all votes are neutral votes, we will not try to predict neutral votes.
This is in line with the Wikipedia RfA process where neutral votes are not counted for the support percentage. 
However, we will use the neutral votes to gather the triadic features and can utilize the additional information to predict votes.
As we discussed in Section~\ref{sec:voting-signed-networks}, the graph combination model is an extension of the sign prediction models for the voting setting.
A major requirement to predict votes is to ensure that there is no \textit{data leakage} when creating the training features $\mathbf{X}$, similar to the process used by Kairimi et al.\ \cite{karimi2019multicongress}.
A data leak is when we have information about the future present in the training data.
This can cause the model that we train to overfit on the leaked data and not generalize.
In our problem setting, we divide the whole \wikirfa into three parts, namely \textit{dev}, \textit{train} and \textit{test}.
As we are predicting votes, we will split the datasets based on the number of votes chronologically and round up to the closest RfA so that it is contiguous. 

The \textit{dev} (or development) dataset will be the set of RfAs which we use to construct the auxiliary and signed graphs.
We ensure that the \usercontrib dataset is also restricted to the edits that happened until the date of the last RfA in the dev dataset.
Therefore, all the five auxiliary graphs and the signed graphs are created only with information that is present in the time frame of the dev dataset.

Next, the \textit{train} (or training) dataset is what we use to create the feature matrix $\mathbf{X}$ and target matrix $\mathbf{y}$.
In this dataset, we only consider the support and oppose votes to be part of the prediction task and hence, filter out all the neutral votes.
Now, for each vote, we create the auxiliary feature vector $\textbf{a}$ using the five auxiliary graphs and the triadic feature vector $t$ from the signed voting graph as described in Algorithms~\ref{alg:auxiliary-feature} and \ref{alg:triad-feature} respectively.
These features are concatenated into a feature vector $\textbf{x}$, which is of length $41$ ($5$ auxiliary and $36$ triadic) and is a row in the feature matrix.
The corresponding true vote is also collected in the target $y_i$.
The \textit{dev} and \textit{train} splits ensure that the auxiliary and triadic features are strictly not overlapping.
This allows the feature matrix $\textbf{X}$ to be independent of time and therefore, be cross validated.

Lastly, the \textit{test} dataset contains votes that the model trained on the training dataset would not have seen and can be used to evaluate the performance of the model.
The feature matrix for the test dataset, $\mathbf{X}_{test}$ and the target, $\mathbf{y}_{test}$ are also constructed in a similar manner.
For each vote in the test dataset, we gather the auxiliary and triadic features from the same graphs as we used for the training phase.
We create each row of $\mathbf{X}_{test}$ by concatenating these features and gathering the true vote as the target.

\subsection{Supervised Classification}
Once we have created the training and testing feature matrices, $\mathbf{X}$ and $\mathbf{X}_{test}$ and the target vectors, $\mathbf{y}$ and $\mathbf{y}_{test}$, the task is a regular supervised classification problem.
We can use any traditional linear classification models such as support vector classifiers (e.g.,linear SVC), logistic regression models or gradient boosting methods (e.g., XGBoost).
We choose a logistic regression (LR) model for its interoperability and robustness to overfitting. 

Given a feature vector $\mathbf{x}=(x_{1},x_{2},\dots,x_{n})$ with $n$ features, a logistic regression model learn to predict the probability of the form 
\begin{equation}
    P(\text{support} \mid \mathbf{x}) = \frac{1}{1+e^{-(\beta_{0} + \boldsymbol{\beta}\mathbf{x}})}
\end{equation}
Where $\beta_{0}$ and $\boldsymbol{\beta} = (\beta_{1},\beta_{2},\dots,\beta_{n})$ are the coefficients that the model learns using the training data.

The \wikirfa dataset has a class imbalance problem.
Support votes are $73\%$ compared to oppose votes at $20\%$.
Therefore, we will utilize class weights inversely proportional to the class frequencies while training so that the model learns to predict negative votes effectively. 
As the training features $\mathbf{X}$ are independent of time, we acn use $k$-fold cross validation to tune the regularization parameter for the logistic regression model.  

\section{Local Signed Network Models}
\label{sec:local-signed-network-implementation}
We now discuss the implementation and significance of the local signed network models discussed in Section~\ref{sec:local-signed-network-theory} to predict votes in Wikipedia RfAs.

These models are iterative models and an important feature is that they are unsupervised.
Therefore, they do not require any learning of parameters or preparation of data for training.
Consequently, we can bootstrap the models to start from the first available RfA.
We achieve this by beginning with an empty relationship graph $R$.
In the first RfA, the LSN network for all the votes contain only the nodes for voter $v$ and candidate $c$.
Therefore, the model will predict all votes with probability $0.5$ of being support votes, as there is no information available.
After the first RfA is over, the relationship graph $R$ will be updated with the voting details.
Now, in the second RfA there is more information present and the model can be predict votes with more certainty.
In this manner,the models can iteratively learn and predict all the votes present in the \wikirfa dataset.

In a similar fashion, the iterative models elegantly handle new users for whom we have no information .
If at any point the current voter $v$ is new and there is no information in the relationship graph , then the model predicts support vote probability of $0.5$, because the LSN contains only the nodes $v$ and $c$ .
This new voter is then integrated into the relationship graph when it is updated after the RfA session, shown in Line~\ref{iterative-pred:line:update-relation}.
Therefore, this new voter's information is now available for future vote predictions.

In this thesis, we wish to separately study the votes that are predicted with no information.
Therefore, in our implementation we specifically mark these votes.
Firstly, we can accurately evaluate the models using only on the votes predicted with information.
Next, we can analyse the distribution of the informationless votes and devise strategies to effectively guess the vote in the case when it is a new user.
Lastly, we can verify if there is some herd mentality when new voters vote for the first time.

We proposed two models, one using balance theory and another using status theory.
Both the models make use of only the election data, therefore, we will the \wikirfa dataset.
First, we describe the iterative balance model and define the relationship graph based on agreement between voters in Wikipedia RfAs.
Secondly, we explain the iterative status model and the relationship graph based on follower ratio in RfAs.

\subsection{Iterative Balance Model}
The \textit{Iterative Balance} model uses balance theory of the local signed network to predict the independent vote in a Wikipedia RfA.
As discussed in Section~\ref{subsec:prediction-based-balance}, we require a signed symmetric measure between two voters.
We now propose a measure based on the \textit{agreement ratio} between two users $u$ and $v$.
The ratio is the number of times $u$ and $v$ have voted the same divided by the number of common RfAs they have participated in.
For example, if $u$ and $v$ have participated in $12$ common RfAs and have voted the same in $9$ RfAs, then the agreement ratio is $0.75$, indicating that they agree more than they disagree.
Therefore, if they have an agreement ratio of $0.5$, then they neither agree or disagree to a significant degree.
The agreement ratio is symmetric and we can covert into a signed measure by subtracting 0.5 from the ratio.

Hence, we define a signed undirected \textit{agreement graph} $A= (V_{A},E_{A},w_{A})$, where the weight function is defined as seen in Equation~\eqref{eqn:agree-weight}.
\begin{equation}
    \label{eqn:agree-weight}
    w_{A}((u,v)) = \frac{\text{Number of times } u \text{ and } v \text{ have voted the same}}{\text{Number of common RfAs for } u \text{ and } v} -0.5
\end{equation}

This agreement graph $A$ is the relationship graph $R$ for the iterative balance model described in Algorithm~\ref{alg:iterative-pred}. 
In Line~\ref{iterative-pred:line:update-relation} there is a a method, $Update(R,S)$ to update the relationship graph after the end of a voting session.
Therefore, we require a method to update the signed weights in the agreement graph $A$ given the RfA voting details in a session $S$.

For notational ease, we assume that each edge $e = (u,v) \in E_{A}$ contains two attributes, $e.agree$ and $e.common$, the agreement ratio and the number of common RfAs between the nodes respectively.
Then, once we get the voting information from the session, we can update the agreement ratio and the number of common RfAs in a straightforward manner.
This process is shown in Algorithm~\ref{alg:agree-update}.
We can bootstrap the model by beginning with an empty agreement graph $A$.
Then for votes with no information the predicted support probability is $0.5$.
After the RfA session is over, these new voters can be incorporated into the agreement graph when it is updated, as seen in Line~\ref{alg:agree-update:new-edge}.

\begin{algorithm}[H]
    \DontPrintSemicolon
    \caption{Update Agree graph after a session}
    \label{alg:agree-update}
    \KwIn{Session graph $S$, Candidate $c$, Agree graph $A$ }
    \KwResult{Updated Agree graph $A$ }
    \tcp{Get all voters}
    $O \leftarrow V_{S}-\{c\}$\;
    Order $O$ by timestamp\;
    \For{$v \in O$}{
        $vote_{v}\leftarrow w_{S}((v,c))$\;
        \ForEach{$u$ who voted after $v$}{
            $vote_{u} \leftarrow w_{S}((u,c))$\;
            $e \leftarrow (v,u)$\;
            \uIf{$e \in E_{A}$}{
                $agree \leftarrow e.agree$\;
                $common \leftarrow e.common$\;
                \eIf{$vote_{v}=vote_{u}$}{
                    $agree \leftarrow ((agree\cdot common) +1)/(common+1)$\;
                }{
                    $agree \leftarrow (agree\cdot common)/(common+1)$\;
                }
                $common \leftarrow common +1$\;
            }
            \ElseIf{$vote_{u}=vote_{v}$}{
            \tcp{if $e$ is a new edge}
            \label{alg:agree-update:new-edge}
            $common \leftarrow $ number of elections $v$ and $u$ have in common\;
            $agree \leftarrow 1/common$\;
            $E_{A} \leftarrow E_{A} \cup \{e\}$
            }
            $e.agree \leftarrow agree$\;
            $e.common \leftarrow common$\;  
            $w_{A}(e) \leftarrow e.agree -0.5$\;
        }
    }
    \Return $A$
\end{algorithm}




    \subsection{Iterative Status Model}
        \begin{itemize} 
            \item Describe how status is derived from the Follows graph in a local signed network 
            \item Discuss Agony algorithm used as blackbox
            \item Discuss how the Follows graph is updated after every election
            \item Describe the probabilistic results.
        \end{itemize}

\section{Voting Order Experiments}
\label{sec:voting-order}


\section{Evaluation Metrics}
\label{sec:eval-metrics}
\begin{itemize}
    \item Discuss the issues with the imbalance in the dataset
    \item Illustrate the issues with accuracy
    \item Discuss ROC and Precision Recall curves for probability based predictions 
    \item Discuss AUC ROC and AUC posPR and AUC negPR
\end{itemize}